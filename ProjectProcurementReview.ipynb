{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4329b8b7-d7a7-449b-81b7-af351449627b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Daiga.Deksne\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Line magic functions that will allow for imports to be reloaded and not cached\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from datetime import date\n",
    "\n",
    "# Local\n",
    "from scripts.extractmd import Extractor\n",
    "from scripts.vectorindex import QnAEngine\n",
    "from scripts.utilities import get_prompt_dict, get_questions, get_answers, get_procurement_content, get_config_data\n",
    "from scripts.gen_results import gen_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f04cb-807c-48b2-b474-8b97dbb0a6d5",
   "metadata": {},
   "source": [
    "**Global config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a282ce0b-1aba-417c-bf9a-15cbe840c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_conf = {\n",
    "    \"embeddingmodel\": \"BAAI/bge-m3\",  # \"BAAI/bge-m3\" \"nomic-ai/nomic-embed-text-v2-moe\" # \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "    \"chunk_size\": 1536,\n",
    "    \"chunk_overlap\": 0,\n",
    "    \"top_similar\": 5,\n",
    "    \"n4rerank\": 0, #How many nodes to retrieve for reranking. If 0, reranker is not used\n",
    "    \"use_similar_chunks\": True, #To use similar chunks or the whole document as the context\n",
    "    \"prevnext\": True #to include in the context also the previouse and the next chunk of the current similar chunk\n",
    "}\n",
    "embedding=HuggingFaceEmbedding(model_name=embedding_conf[\"embeddingmodel\"],trust_remote_code=True)\n",
    "\n",
    "#For nomic-embed-text-v2-moe\n",
    "#embedding=HuggingFaceEmbedding(model_name=embedding_conf[\"embeddingmodel\"],trust_remote_code=True,query_instruction=\"search_query: \",text_instruction=\"search_document: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e114f1-c05e-48b1-b56a-051ece6102eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Setup \n",
    "llmmodelAzure = { \"model\": \"gpt-4o\",\n",
    "                \"version\":os.environ.get('AZURE_OPENAI_VERSION',''),\n",
    "                \"azure_deployment\":\"gpt-4o\",\n",
    "                \"azure_endpoint\":os.environ.get('AZURE_ENDPOINT',''),\n",
    "                \"api_key\":os.environ.get('AZURE_OPENAI_KEY','')}\n",
    "\n",
    "llm=AzureOpenAI(azure_deployment=llmmodelAzure[\"azure_deployment\"],\n",
    "                azure_endpoint=llmmodelAzure[\"azure_endpoint\"],temperature=0.0,\n",
    "                api_version=llmmodelAzure[\"version\"], api_key=llmmodelAzure[\"api_key\"],\n",
    "                timeout=120,max_retries=3,top_p=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f7c744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qnaengine = QnAEngine(embedding,llm)\n",
    "extractor = Extractor() # Markdown doc extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e67876-c437-41ad-992b-4ce540022087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ollama model\n",
    "# llmmodelOllama = { \"model\": \"gemma3:27b\",\n",
    "#                 \"url\":os.environ.get('OLLAMA_ENDPOINT',''),\n",
    "#                 \"context_window\":\"128000\"}\n",
    "\n",
    "#from llama_index.llms.ollama import Ollama\n",
    "#llm = Ollama(base_url=llmmodelOllama[\"url\"],\n",
    "#             model=llmmodelOllama[\"model\"], \n",
    "#             context_window=int(llmmodelOllama[\"context_window\"]),\n",
    "#            request_timeout=300.0,\n",
    "#            temperature=0.0,\n",
    "#            additional_kwargs={\"seed\":1337})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0daa1-7706-4651-b713-78ac66c5533a",
   "metadata": {},
   "source": [
    "**PROCUREMENT FILE SETTINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88446367-533a-4e00-b3ff-239802a86698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions loaded\n"
     ]
    }
   ],
   "source": [
    "# Script dir for getting relative paths for notebook file\n",
    "script_dir = globals()['_dh'][0] \n",
    "\n",
    "# Document paths\n",
    "question_file_path = script_dir / \"questions\" / \"questions.yaml\"\n",
    "prompt_file = script_dir / \"questions\" / \"prompts.tsv\"\n",
    "report_dir = script_dir / \"reports\"\n",
    "config_dir = script_dir / \"config\"\n",
    "procurement_file_dir = script_dir / \"cfla_files\"\n",
    "answer_file_dir = script_dir / \"answers\"\n",
    "\n",
    "# Loading static information TODO\n",
    "question_dictionary = get_questions(question_file_path)\n",
    "promptdict = get_prompt_dict(prompt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78357252",
   "metadata": {},
   "source": [
    "**MAIN Q/A GENERATION SCRIPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f5149a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31 config files in C:\\Repos\\vpp-cfla\\config\n",
      "Processing config file: C:\\Repos\\vpp-cfla\\config\\APP_DI_20202ERAF_AK.ini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████████████████████████████████████████████████████████████████ [ time left: 00:00 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 segments created and vectorized.\n",
      "Index is ready.\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 "
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'answer0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m qnaengine\u001b[38;5;241m.\u001b[39mcreateIndex(\n\u001b[0;32m     19\u001b[0m         procurement_content,\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcurement\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m         chunk_size\u001b[38;5;241m=\u001b[39membedding_conf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_size\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     22\u001b[0m         chunk_overlap\u001b[38;5;241m=\u001b[39membedding_conf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_overlap\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     23\u001b[0m         )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Generating results\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m results_table \u001b[38;5;241m=\u001b[39m gen_results(qnaengine, configfile, embedding_conf, question_dictionary, answer_dictionary, promptdict)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Save output\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# TODO probably should merge the output file together; One output file for the entire run, that contains each specific procurement results\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# TODO maybe implement some crash prevention strategy, so if some file breaks it doesn't break the entire run\u001b[39;00m\n\u001b[0;32m     32\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results_table, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAtbilde\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSagaidāmā atbilde\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPamatojums\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mC:\\Repos\\vpp-cfla\\scripts\\gen_results.py:32\u001b[0m, in \u001b[0;36mgen_results\u001b[1;34m(qnaengine, configfile, embedding_conf, question_dictonary, answer_dictonary, promptdict)\u001b[0m\n\u001b[0;32m     24\u001b[0m extrainfo \u001b[38;5;241m=\u001b[39m qnaengine\u001b[38;5;241m.\u001b[39mcompressPrompt(extrainfo,\u001b[38;5;241m3000\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m singleq:\n\u001b[0;32m     27\u001b[0m     result0 \u001b[38;5;241m=\u001b[39m ask_question_save_answer(qnaengine, \n\u001b[0;32m     28\u001b[0m                                        embedding_conf, \n\u001b[0;32m     29\u001b[0m                                        promptdict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m extrainfo, \n\u001b[0;32m     30\u001b[0m                                        singleq[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion0\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m     31\u001b[0m                                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msingleq[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnr\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m---> 32\u001b[0m                                        \u001b[43msinglea\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manswer0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     33\u001b[0m     results_table\u001b[38;5;241m.\u001b[39mappend(result0)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m results_table[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m results_table[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m2\u001b[39m]:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'answer0'"
     ]
    }
   ],
   "source": [
    "# TODO config file loop; For each config in config_folder_path; # TODO add parallel prompting\n",
    "# Create an array of ini files and then call them in a loop; This will give us more control over which files to call/exclude\n",
    "\n",
    "ini_files = [f for f in os.listdir(config_dir) if f.endswith('.ini')]\n",
    "print(f\"Found {len(ini_files)} config files in {config_dir}\")\n",
    "\n",
    "# TODO add a little indicator how many files are to be processes; + aproximate time to finish each file\n",
    "for file in ini_files:\n",
    "        configfile = config_dir / file\n",
    "        print(f\"Processing config file: {configfile}\")\n",
    "        procurement_id, procurement_file, agreement_file, answer_file = get_config_data(configfile, procurement_file_dir, answer_file_dir)\n",
    "        answer_dictionary = get_answers(answer_file)\n",
    "\n",
    "        # Getting markdown text from procurement doc\n",
    "        procurement_content = get_procurement_content(extractor, procurement_file, agreement_file)\n",
    "    \n",
    "        # Creating FAISS vector index for the procurement document\n",
    "        await qnaengine.createIndex(\n",
    "                procurement_content,\n",
    "                \"Procurement\",\n",
    "                chunk_size=embedding_conf[\"chunk_size\"],\n",
    "                chunk_overlap=embedding_conf[\"chunk_overlap\"]\n",
    "                )\n",
    "\n",
    "        # Generating results\n",
    "        results_table = gen_results(qnaengine, configfile, embedding_conf, question_dictionary, answer_dictionary, promptdict)\n",
    "\n",
    "        # Save output\n",
    "\n",
    "        # TODO probably should merge the output file together; One output file for the entire run, that contains each specific procurement results\n",
    "        # TODO maybe implement some crash prevention strategy, so if some file breaks it doesn't break the entire run\n",
    "        data = pd.DataFrame(results_table, columns=[\"Nr\", \"Atbilde\", \"Sagaidāmā atbilde\", \"Pamatojums\"])\n",
    "        precision = (data['Atbilde'] == data['Sagaidāmā atbilde']).sum()/len(data)\n",
    "        print(f\"PRECIZITĀTE: {precision*100}%\")\n",
    "\n",
    "        # Save final output file\n",
    "        with open(f\"{report_dir}\\{date.today():%d.%m}_{procurement_id.replace('/','_')}.htm\", 'w', encoding='utf-8') as ofile:\n",
    "                print(data.to_html(index=False).replace('\\\\n','<br>'),file=ofile)\n",
    "                print(f\"PRECIZITĀTE: {precision*100}%\",file=ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fde67c-e73e-41d2-924d-af88221bfcc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
