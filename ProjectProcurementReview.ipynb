{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4329b8b7-d7a7-449b-81b7-af351449627b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Line magic functions that will allow for imports to be reloaded and not cached\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "from json import dump\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# Local\n",
    "from scripts.extractmd import Extractor\n",
    "from scripts.vectorindex import QnAEngine\n",
    "from scripts.utilities import get_prompt_dict, get_questions, get_answers, get_procurement_content, get_config_data, get_ini_files\n",
    "from scripts.gen_results import gen_results\n",
    "from scripts.gen_reports import generate_precision_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f04cb-807c-48b2-b474-8b97dbb0a6d5",
   "metadata": {},
   "source": [
    "**Global config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a282ce0b-1aba-417c-bf9a-15cbe840c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_conf = {\n",
    "    \"embeddingmodel\": \"BAAI/bge-m3\",  # \"BAAI/bge-m3\" \"nomic-ai/nomic-embed-text-v2-moe\" # \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "    \"chunk_size\": 1536,\n",
    "    \"chunk_overlap\": 0,\n",
    "    \"top_similar\": 5,\n",
    "    \"n4rerank\": 0, #How many nodes to retrieve for reranking. If 0, reranker is not used\n",
    "    \"use_similar_chunks\": True, #To use similar chunks or the whole document as the context\n",
    "    \"prevnext\": True #to include in the context also the previouse and the next chunk of the current similar chunk\n",
    "}\n",
    "embedding=HuggingFaceEmbedding(model_name=embedding_conf[\"embeddingmodel\"],trust_remote_code=True)\n",
    "\n",
    "#For nomic-embed-text-v2-moe\n",
    "#embedding=HuggingFaceEmbedding(model_name=embedding_conf[\"embeddingmodel\"],trust_remote_code=True,query_instruction=\"search_query: \",text_instruction=\"search_document: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e114f1-c05e-48b1-b56a-051ece6102eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt-4o', 'version': '2025-01-01-preview', 'azure_deployment': 'gpt-4o', 'azure_endpoint': 'https://vpp-cfla-llm.openai.azure.com/', 'api_key': 'F7Rj7q0vZinLWHNK9kBcmFuqnctsrrifekq4E7u5V867VOdyHtJPJQQJ99BEACfhMk5XJ3w3AAABACOGIwwk'}\n"
     ]
    }
   ],
   "source": [
    "# LLM Setup \n",
    "llmmodelAzure = { \"model\": \"gpt-4o\",\n",
    "                \"version\":os.environ.get('AZURE_OPENAI_VERSION',''),\n",
    "                \"azure_deployment\":\"gpt-4o\",\n",
    "                \"azure_endpoint\":os.environ.get('AZURE_ENDPOINT',''),\n",
    "                \"api_key\":os.environ.get('AZURE_OPENAI_KEY','')}\n",
    "\n",
    "llm=AzureOpenAI(azure_deployment=llmmodelAzure[\"azure_deployment\"],\n",
    "                azure_endpoint=llmmodelAzure[\"azure_endpoint\"],temperature=0.0,\n",
    "                api_version=llmmodelAzure[\"version\"], api_key=llmmodelAzure[\"api_key\"],\n",
    "                timeout=120,max_retries=3,top_p=0.0001)\n",
    "print(llmmodelAzure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f7c744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = Extractor() # Markdown doc extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e67876-c437-41ad-992b-4ce540022087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ollama model\n",
    "# llmmodelOllama = { \"model\": \"gemma3:27b\",\n",
    "#                 \"url\":os.environ.get('OLLAMA_ENDPOINT',''),\n",
    "#                 \"context_window\":\"128000\"}\n",
    "\n",
    "#from llama_index.llms.ollama import Ollama\n",
    "#llm = Ollama(base_url=llmmodelOllama[\"url\"],\n",
    "#             model=llmmodelOllama[\"model\"], \n",
    "#             context_window=int(llmmodelOllama[\"context_window\"]),\n",
    "#            request_timeout=300.0,\n",
    "#            temperature=0.0,\n",
    "#            additional_kwargs={\"seed\":1337})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0daa1-7706-4651-b713-78ac66c5533a",
   "metadata": {},
   "source": [
    "**PROCUREMENT FILE SETTINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88446367-533a-4e00-b3ff-239802a86698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script dir for getting relative paths for notebook file\n",
    "script_dir = globals()['_dh'][0] \n",
    "\n",
    "# Document paths\n",
    "question_file_path = script_dir / \"questions\" / \"questions.yaml\"\n",
    "prompt_file = script_dir / \"questions\" / \"prompts.tsv\"\n",
    "report_dir = script_dir / \"reports\"\n",
    "config_dir = script_dir / \"config\" # \"dev_config\" # \"config\"\n",
    "procurement_file_dir = script_dir / \"cfla_files\" # \"cfla_files\"\n",
    "answer_file_dir = script_dir / \"answers\"\n",
    "\n",
    "# TODO perhaps prompt user to define unique report name; some types - all; one etc?\n",
    "report_identifier = \"final\"\n",
    "# TODO maybe add report as a subdirectory as there are 2 files per report; might be even more with histograms etc.\n",
    "report_name = f\"{report_identifier}_{date.today():%d.%m}\"\n",
    "\n",
    "report_dir_path = report_dir / report_name\n",
    "report_path_htm = report_dir_path / \"report.htm\"\n",
    "report_path_csv = report_dir_path / \"report.csv\"\n",
    "report_path_config = report_dir_path / \"config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e29fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading static information\n",
    "overwrite = False  # If true this will delete the existing report and generate a new one;\n",
    "                  # Else - new data will be appended only if it isn't in the CSV file.\n",
    "\n",
    "question_dictionary = get_questions(question_file_path)\n",
    "prompt_dictionary = get_prompt_dict(prompt_file)\n",
    "\n",
    "ini_files = get_ini_files(config_dir, overwrite, report_path_csv)\n",
    "print(f\"Processing {len(ini_files)} procurement files: {sorted(ini_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78357252",
   "metadata": {},
   "source": [
    "**MAIN Q/A GENERATION SCRIPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5149a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9682a43e5d848078d9b8d3027d09fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Config files:   0%|          | 0/2 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing config file: c:\\Users\\Admin\\Desktop\\Programming\\Work\\vpp-cfla\\config\\RPNC202122.ini\n",
      "Loaded layout model s3://layout/2025_02_18 on device cpu with dtype torch.float32\n",
      "Loaded texify model s3://texify/2025_02_18 on device cpu with dtype torch.float32\n",
      "Loaded recognition model s3://text_recognition/2025_02_18 on device cpu with dtype torch.float32\n",
      "Loaded table recognition model s3://table_recognition/2025_02_18 on device cpu with dtype torch.float32\n",
      "Loaded detection model s3://text_detection/2025_02_28 on device cpu with dtype torch.float32\n",
      "Loaded detection model s3://inline_math_detection/2025_02_24 on device cpu with dtype torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recognizing layout: 100%|██████████| 4/4 [00:21<00:00,  5.33s/it]\n",
      "Running OCR Error Detection: 100%|██████████| 6/6 [00:02<00:00,  2.54it/s]\n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n",
      "c:\\Users\\Admin\\Desktop\\Programming\\Work\\vpp-cfla\\.venv\\Lib\\site-packages\\threadpoolctl.py:1226: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n",
      "Recognizing tables: 100%|██████████| 5/5 [00:34<00:00,  6.93s/it]\n",
      "Generating embeddings: 100%|██████████ [ time left: 00:00 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 segments created and vectorized.\n",
      "Index is ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Questions in /home/rud/CFLA/vpp-cfla/demo_config/GNP202565.ini:   0%|          | 0/68 [00:00<?, ?q/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 "
     ]
    }
   ],
   "source": [
    "# TODO add parallel prompting\n",
    "\n",
    "if overwrite: # overwrtitting report; Delete and create new\n",
    "        if report_path_htm.exists():\n",
    "                report_path_htm.unlink()\n",
    "        if report_path_csv.exists():\n",
    "                report_path_csv.unlink()\n",
    "        if report_path_config.exists():\n",
    "                report_path_config.unlink()\n",
    "                \n",
    "if not os.path.exists(report_dir_path):\n",
    "        os.makedirs(report_dir_path)\n",
    "\n",
    "# Make config dictionary and save as json\n",
    "config_dict = embedding_conf\n",
    "config_dict[\"model\"] = llmmodelAzure[\"model\"]\n",
    "config_dict[\"temperature\"] = llm.temperature\n",
    "config_dict[\"temperature\"] = llm.timeout\n",
    "config_dict[\"max_retries\"] = llm.max_retries\n",
    "#config_dict[\"top_p\"] = 0.0001\n",
    "\n",
    "with open(report_path_config, \"w\") as file:\n",
    "        dump(config_dict, file) \n",
    "\n",
    "\n",
    "for file in tqdm(ini_files, desc=\"Config files\", unit=\"file\"): # TODO fix TQDM; expected that after first file finishes it shows the aproximation of all the other files left to process\n",
    "        configfile = config_dir / f\"{file}.ini\"\n",
    "\n",
    "        #if f\"{file}.ini\" == \"RTK_20209.ini\": continue\n",
    "        #if f\"{file}.ini\" == \"VeA_202012ERAFGroz.ini\": continue\n",
    "        \n",
    "        print(f\"Processing config file: {configfile}\")\n",
    "        procurement_id, procurement_file, agreement_file, answer_file = get_config_data(configfile, procurement_file_dir, answer_file_dir)\n",
    "        # Open CSV file, maybe as pandas dataframe\n",
    "        answer_dictionary = get_answers(answer_file)\n",
    "        print(f\"Processing config file: {configfile}\")\n",
    "\n",
    "        # Getting markdown text from procurement doc\n",
    "        procurement_content = get_procurement_content(extractor, procurement_file, agreement_file)\n",
    "    \n",
    "        # Creating FAISS vector index for the procurement document\n",
    "        qnaengine = QnAEngine(embedding,llm)\n",
    "        await qnaengine.createIndex(\n",
    "                procurement_content,\n",
    "                \"Procurement\",\n",
    "                chunk_size=embedding_conf[\"chunk_size\"],\n",
    "                chunk_overlap=embedding_conf[\"chunk_overlap\"]\n",
    "                )\n",
    "\n",
    "        ### Generating results\n",
    "        results_table = gen_results(qnaengine, configfile, embedding_conf, question_dictionary, answer_dictionary, prompt_dictionary)\n",
    "        \n",
    "        # add \"Iepirkuma ID\" as procurement_id to results table\n",
    "        # TODO move this inside gen results function once it has been refactored\n",
    "        for row in results_table:\n",
    "                row.insert(0, file)\n",
    "        \n",
    "        ### Save output\n",
    "        data = pd.DataFrame(results_table, columns=[\"Iepirkuma ID\", \"Nr\", \"Atbilde\", \"Sagaidāmā atbilde\", \"Pamatojums\"])\n",
    "        precision = (data['Atbilde'] == data['Sagaidāmā atbilde']).sum()/len(data)\n",
    "        print(f\"PRECIZITĀTE: {precision*100}%\")\n",
    "\n",
    "        with report_path_htm.open('a', encoding='utf-8') as ofile:\n",
    "                # TODO Create a dropdown menu that lets the user select file by \"Iepirkuma ID\" - in each page only information that has that \"Iepirkuma ID\" is displayed\n",
    "                print(procurement_id,file=ofile)\n",
    "                print(data.to_html(index=False).replace('\\\\n','<br>'),file=ofile)\n",
    "                print(f\"PRECIZITĀTE: {precision*100}%\",file=ofile)\n",
    "\n",
    "        data.to_csv(report_path_csv, \n",
    "                    mode='a', \n",
    "                    index=False, \n",
    "                    header=not report_path_csv.exists(), # only adding one header\n",
    "                    encoding='utf-8')\n",
    "        \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cbe6e6",
   "metadata": {},
   "source": [
    "# Question precision data report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3f08f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML report saved to: /home/andris/src/vpp-cfla/reports/all_26.06/precision_report.html\n"
     ]
    }
   ],
   "source": [
    "# Load report we want to analayze\n",
    "analyzed_report_dir = report_dir/ \"all_26.06\"\n",
    "csv_filename = \"report.csv\"\n",
    "input_csv = analyzed_report_dir / csv_filename\n",
    "\n",
    "# Generate report\n",
    "precison_report_html = generate_precision_report(input_csv)\n",
    "\n",
    "# Save the report next to the CSV\n",
    "output_html = analyzed_report_dir / f\"precision_report.html\"\n",
    "with open(output_html, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(precison_report_html)\n",
    "\n",
    "print(f\"HTML report saved to: {output_html}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593a08d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO confusion matrix (false positives; true positives etc)\n",
    "\n",
    "# TODO Make view for specific procurmennt\n",
    "# report.htm gets generated after from csv.\n",
    "# There is a dropdown that can be used to select which report we want to see"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
