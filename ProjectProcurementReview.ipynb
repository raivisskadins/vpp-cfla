{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4329b8b7-d7a7-449b-81b7-af351449627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line magic functions that will allow for imports to be reloaded and not cached\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "from json import dump\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# Local\n",
    "from scripts.extractmd import Extractor\n",
    "from scripts.vectorindex import QnAEngine\n",
    "from scripts.utilities import get_prompt_dict, get_questions, get_answers, get_procurement_content, get_config_data, get_ini_files\n",
    "from scripts.gen_results import gen_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f04cb-807c-48b2-b474-8b97dbb0a6d5",
   "metadata": {},
   "source": [
    "**Global config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a282ce0b-1aba-417c-bf9a-15cbe840c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_conf = {\n",
    "    \"embeddingmodel\": \"BAAI/bge-m3\",  # \"BAAI/bge-m3\" \"nomic-ai/nomic-embed-text-v2-moe\" # \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "    \"chunk_size\": 1536,\n",
    "    \"chunk_overlap\": 0,\n",
    "    \"top_similar\": 5,\n",
    "    \"n4rerank\": 0, #How many nodes to retrieve for reranking. If 0, reranker is not used\n",
    "    \"use_similar_chunks\": True, #To use similar chunks or the whole document as the context\n",
    "    \"prevnext\": True #to include in the context also the previouse and the next chunk of the current similar chunk\n",
    "}\n",
    "embedding=HuggingFaceEmbedding(model_name=embedding_conf[\"embeddingmodel\"],trust_remote_code=True)\n",
    "\n",
    "#For nomic-embed-text-v2-moe\n",
    "#embedding=HuggingFaceEmbedding(model_name=embedding_conf[\"embeddingmodel\"],trust_remote_code=True,query_instruction=\"search_query: \",text_instruction=\"search_document: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e114f1-c05e-48b1-b56a-051ece6102eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt-4o', 'version': '2025-01-01-preview', 'azure_deployment': 'gpt-4o', 'azure_endpoint': 'https://vpp-cfla-llm.openai.azure.com/', 'api_key': 'F7Rj7q0vZinLWHNK9kBcmFuqnctsrrifekq4E7u5V867VOdyHtJPJQQJ99BEACfhMk5XJ3w3AAABACOGIwwk'}\n"
     ]
    }
   ],
   "source": [
    "# LLM Setup \n",
    "llmmodelAzure = { \"model\": \"gpt-4o\",\n",
    "                \"version\":os.environ.get('AZURE_OPENAI_VERSION',''),\n",
    "                \"azure_deployment\":\"gpt-4o\",\n",
    "                \"azure_endpoint\":os.environ.get('AZURE_ENDPOINT',''),\n",
    "                \"api_key\":os.environ.get('AZURE_OPENAI_KEY','')}\n",
    "\n",
    "llm=AzureOpenAI(azure_deployment=llmmodelAzure[\"azure_deployment\"],\n",
    "                azure_endpoint=llmmodelAzure[\"azure_endpoint\"],temperature=0.0,\n",
    "                api_version=llmmodelAzure[\"version\"], api_key=llmmodelAzure[\"api_key\"],\n",
    "                timeout=120,max_retries=3,top_p=0.0001)\n",
    "print(llmmodelAzure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f7c744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = Extractor() # Markdown doc extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e67876-c437-41ad-992b-4ce540022087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ollama model\n",
    "# llmmodelOllama = { \"model\": \"gemma3:27b\",\n",
    "#                 \"url\":os.environ.get('OLLAMA_ENDPOINT',''),\n",
    "#                 \"context_window\":\"128000\"}\n",
    "\n",
    "#from llama_index.llms.ollama import Ollama\n",
    "#llm = Ollama(base_url=llmmodelOllama[\"url\"],\n",
    "#             model=llmmodelOllama[\"model\"], \n",
    "#             context_window=int(llmmodelOllama[\"context_window\"]),\n",
    "#            request_timeout=300.0,\n",
    "#            temperature=0.0,\n",
    "#            additional_kwargs={\"seed\":1337})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0daa1-7706-4651-b713-78ac66c5533a",
   "metadata": {},
   "source": [
    "**PROCUREMENT FILE SETTINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88446367-533a-4e00-b3ff-239802a86698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions loaded\n",
      "Found 27 config files in c:\\Users\\Admin\\Desktop\\Programming\\Work\\vpp-cfla\\config\n",
      "Skipping 25 already-processed files: ['APP_DI_20202ERAF_AK', 'DND_20206', 'EDI_20203AK', 'IKVD_20213_ESF_SAM', 'IP2020_01_ERAF', 'IeM_IC_201913', 'KNP202134', 'KNP_202215', 'KP2020016A-KF', 'LNP_201976', 'LNP_202037ERAF', 'LNP_202050ERAF', 'LU_CFI_201935ERAF', 'MNP_202121_ERAF', 'PND_2019_15-ERAF', 'RTK_2019_12', 'SND_202015-ERAF', 'SNP202131', 'SNP_202001', 'SNP_202067', 'SNP_20213ERAF', 'VND_20201', 'VNIP_2020_036_ERAF', 'VNP_2023057AK', 'VeA_202012ERAF']\n",
      "Processing 2 procurement files: ['LU_202049_ERAF', 'RPNC202122']\n"
     ]
    }
   ],
   "source": [
    "overwrite = False  # If true this will delete the existing report and generate a new one;\n",
    "                  # Else - new data will be appended only if it isn't in the CSV file.\n",
    "\n",
    "# Script dir for getting relative paths for notebook file\n",
    "script_dir = globals()['_dh'][0] \n",
    "\n",
    "# Document paths\n",
    "question_file_path = script_dir / \"questions\" / \"questions.yaml\"\n",
    "prompt_file = script_dir / \"questions\" / \"prompts.tsv\"\n",
    "report_dir = script_dir / \"reports\"\n",
    "config_dir = script_dir / \"config\" # \"dev_config\" # \"config\"\n",
    "procurement_file_dir = script_dir / \"cfla_files\" # \"cfla_files\"\n",
    "answer_file_dir = script_dir / \"answers\"\n",
    "\n",
    "# TODO perhaps prompt user to define unique report name; some types - all; one etc?\n",
    "report_identifier = \"final\"\n",
    "# TODO maybe add report as a subdirectory as there are 2 files per report; might be even more with histograms etc.\n",
    "report_name = f\"{report_identifier}_{date.today():%d.%m}\"\n",
    "\n",
    "report_dir_path = report_dir / report_name\n",
    "report_path_htm = report_dir_path / \"report.htm\"\n",
    "report_path_csv = report_dir_path / \"report.csv\"\n",
    "report_path_config = report_dir_path / \"config.json\"\n",
    "\n",
    "# Loading static information\n",
    "question_dictionary = get_questions(question_file_path)\n",
    "prompt_dictionary = get_prompt_dict(prompt_file)\n",
    "\n",
    "ini_files = get_ini_files(config_dir, overwrite, report_path_csv)\n",
    "print(f\"Processing {len(ini_files)} procurement files: {sorted(ini_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78357252",
   "metadata": {},
   "source": [
    "**MAIN Q/A GENERATION SCRIPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5149a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9682a43e5d848078d9b8d3027d09fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Config files:   0%|          | 0/2 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing config file: c:\\Users\\Admin\\Desktop\\Programming\\Work\\vpp-cfla\\config\\RPNC202122.ini\n",
      "Loaded layout model s3://layout/2025_02_18 on device cpu with dtype torch.float32\n",
      "Loaded texify model s3://texify/2025_02_18 on device cpu with dtype torch.float32\n",
      "Loaded recognition model s3://text_recognition/2025_02_18 on device cpu with dtype torch.float32\n",
      "Loaded table recognition model s3://table_recognition/2025_02_18 on device cpu with dtype torch.float32\n",
      "Loaded detection model s3://text_detection/2025_02_28 on device cpu with dtype torch.float32\n",
      "Loaded detection model s3://inline_math_detection/2025_02_24 on device cpu with dtype torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recognizing layout: 100%|██████████| 4/4 [00:21<00:00,  5.33s/it]\n",
      "Running OCR Error Detection: 100%|██████████| 6/6 [00:02<00:00,  2.54it/s]\n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n",
      "c:\\Users\\Admin\\Desktop\\Programming\\Work\\vpp-cfla\\.venv\\Lib\\site-packages\\threadpoolctl.py:1226: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n",
      "Recognizing tables: 100%|██████████| 5/5 [00:34<00:00,  6.93s/it]\n",
      "Generating embeddings: 100%|██████████ [ time left: 00:00 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 segments created and vectorized.\n",
      "Index is ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Questions in /home/rud/CFLA/vpp-cfla/demo_config/GNP202565.ini:   0%|          | 0/68 [00:00<?, ?q/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 "
     ]
    }
   ],
   "source": [
    "# TODO add parallel prompting\n",
    "\n",
    "if overwrite: # overwrtitting report; Delete and create new\n",
    "        if report_path_htm.exists():\n",
    "                report_path_htm.unlink()\n",
    "        if report_path_csv.exists():\n",
    "                report_path_csv.unlink()\n",
    "        if report_path_config.exists():\n",
    "                report_path_config.unlink()\n",
    "                \n",
    "if not os.path.exists(report_dir_path):\n",
    "        os.makedirs(report_dir_path)\n",
    "\n",
    "# Make config dictionary and save as json\n",
    "config_dict = embedding_conf\n",
    "config_dict[\"model\"] = llmmodelAzure[\"model\"]\n",
    "config_dict[\"temperature\"] = llm.temperature\n",
    "config_dict[\"temperature\"] = llm.timeout\n",
    "config_dict[\"max_retries\"] = llm.max_retries\n",
    "#config_dict[\"top_p\"] = 0.0001\n",
    "\n",
    "with open(report_path_config, \"w\") as file:\n",
    "        dump(config_dict, file) \n",
    "\n",
    "\n",
    "for file in tqdm(ini_files, desc=\"Config files\", unit=\"file\"): # TODO fix TQDM; expected that after first file finishes it shows the aproximation of all the other files left to process\n",
    "        configfile = config_dir / f\"{file}.ini\"\n",
    "\n",
    "        #if f\"{file}.ini\" == \"RTK_20209.ini\": continue\n",
    "        #if f\"{file}.ini\" == \"VeA_202012ERAFGroz.ini\": continue\n",
    "        \n",
    "        print(f\"Processing config file: {configfile}\")\n",
    "        procurement_id, procurement_file, agreement_file, answer_file = get_config_data(configfile, procurement_file_dir, answer_file_dir)\n",
    "        # Open CSV file, maybe as pandas dataframe\n",
    "        answer_dictionary = get_answers(answer_file)\n",
    "        print(f\"Processing config file: {configfile}\")\n",
    "\n",
    "        # Getting markdown text from procurement doc\n",
    "        procurement_content = get_procurement_content(extractor, procurement_file, agreement_file)\n",
    "    \n",
    "        # Creating FAISS vector index for the procurement document\n",
    "        qnaengine = QnAEngine(embedding,llm)\n",
    "        await qnaengine.createIndex(\n",
    "                procurement_content,\n",
    "                \"Procurement\",\n",
    "                chunk_size=embedding_conf[\"chunk_size\"],\n",
    "                chunk_overlap=embedding_conf[\"chunk_overlap\"]\n",
    "                )\n",
    "\n",
    "        ### Generating results\n",
    "        results_table = gen_results(qnaengine, configfile, embedding_conf, question_dictionary, answer_dictionary, prompt_dictionary)\n",
    "        \n",
    "        # add \"Iepirkuma ID\" as procurement_id to results table\n",
    "        # TODO move this inside gen results function once it has been refactored\n",
    "        for row in results_table:\n",
    "                row.insert(0, file)\n",
    "        \n",
    "        ### Save output\n",
    "        data = pd.DataFrame(results_table, columns=[\"Iepirkuma ID\", \"Nr\", \"Atbilde\", \"Sagaidāmā atbilde\", \"Pamatojums\"])\n",
    "        precision = (data['Atbilde'] == data['Sagaidāmā atbilde']).sum()/len(data)\n",
    "        print(f\"PRECIZITĀTE: {precision*100}%\")\n",
    "\n",
    "        with report_path_htm.open('a', encoding='utf-8') as ofile:\n",
    "                # TODO Create a dropdown menu that lets the user select file by \"Iepirkuma ID\" - in each page only information that has that \"Iepirkuma ID\" is displayed\n",
    "                print(procurement_id,file=ofile)\n",
    "                print(data.to_html(index=False).replace('\\\\n','<br>'),file=ofile)\n",
    "                print(f\"PRECIZITĀTE: {precision*100}%\",file=ofile)\n",
    "\n",
    "        data.to_csv(report_path_csv, \n",
    "                    mode='a', \n",
    "                    index=False, \n",
    "                    header=not report_path_csv.exists(), # only adding one header\n",
    "                    encoding='utf-8')\n",
    "        \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cbe6e6",
   "metadata": {},
   "source": [
    "Table for Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f08f7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'report_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 1) Locate the CSV \u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Use the current working directory for environments where __file__ isn't defined\u001b[39;00m\n\u001b[32m      7\u001b[39m script_dir = Path.cwd()\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m analyzed_report_dir = \u001b[43mreport_dir\u001b[49m/ \u001b[33m\"\u001b[39m\u001b[33mall_26_06\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Specify the CSV filename \u001b[39;00m\n\u001b[32m     11\u001b[39m csv_filename = \u001b[33m\"\u001b[39m\u001b[33mreport.csv\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'report_dir' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Locate the CSV \n",
    "# Use the current working directory for environments where __file__ isn't defined\n",
    "script_dir = Path.cwd()\n",
    "analyzed_report_dir = report_dir/ \"all_26_06\"\n",
    "\n",
    "# Specify the CSV filename \n",
    "csv_filename = \"report.csv\"\n",
    "input_csv = analyzed_report_dir / csv_filename\n",
    "\n",
    "if not input_csv.exists():\n",
    "    raise FileNotFoundError(f\"Cannot find CSV at {input_csv}\")\n",
    "\n",
    "# 2) Read data and mark correctness\n",
    "df = pd.read_csv(input_csv)\n",
    "df['correct'] = df['Atbilde'] == df['Sagaidāmā atbilde']\n",
    "\n",
    "# 3) Compute per-question precision\n",
    "summary = (\n",
    "    df\n",
    "    .groupby('Nr')#TODO GROUP BY NUMBER AS \n",
    "    .agg(\n",
    "        total_asked=('correct', 'size'),\n",
    "        n_correct=('correct', 'sum')\n",
    "    )\n",
    "    .assign(precision=lambda d: d['n_correct'] / d['total_asked'])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 4) Build the HTML report (table only)\n",
    "table_html = summary.to_html(index=False)\n",
    "html = f\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <title>Precision Report</title>\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; padding: 20px; }}\n",
    "        h1 {{ color: #333; }}\n",
    "        table {{ border-collapse: collapse; width: 100%; margin-top: 20px; }}\n",
    "        th, td {{ border: 1px solid #ccc; padding: 8px; text-align: center; }}\n",
    "        th {{ background-color: #f4f4f4; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Model Precision per Question</h1>\n",
    "    {table_html}\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "\n",
    "# 5) Save the report next to the CSV\n",
    "output_html = analyzed_report_dir / f\"precision_report_{csv_filename.replace('.csv', '')}.html\"\n",
    "with open(output_html, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html)\n",
    "\n",
    "print(f\"HTML report saved to: {output_html}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593a08d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remake using CSV; no need for loops just use pandas dataframe to get all the data\n",
    "# Get a dataframe that contains the following:\n",
    "# procurement_id\n",
    "# Question_nr, \n",
    "# Answers_total(this will contain the count of the answers where there were no missing - \"?\" answers),\n",
    "# TODO pielabot tabulas vērtības\n",
    "# Expected_answer,\n",
    "# Yes_count,\n",
    "# No_count,\n",
    "# n_a_count, (Having all of this data could be useful for seeing maybe the llm likes to frequently say there is not enough context for example - problem is with RAG)\n",
    "\n",
    "# TODO Generate final report:\n",
    "# Using above data get the amount of correct answers, better would be a fraction like 7/10 instead of % for readability\n",
    "# Get accuray for each file (%)\n",
    "# Get total accuracy on all questions on all files as %; \n",
    "\n",
    "# Later TODO use the CSVs and dataframes to generate a html file as a nice clean readable report - we still need to discuss this\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
