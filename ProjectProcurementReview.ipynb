{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4329b8b7-d7a7-449b-81b7-af351449627b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Daiga.Deksne\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Line magic functions that will allow for imports to be reloaded and not cached\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from datetime import date\n",
    "\n",
    "# Local\n",
    "from scripts.extractmd import Extractor\n",
    "from scripts.vectorindex import QnAEngine\n",
    "from scripts.utilities import get_prompt_dict, get_questions, get_answers, get_procurement_content, get_config_data\n",
    "from scripts.gen_results import gen_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f04cb-807c-48b2-b474-8b97dbb0a6d5",
   "metadata": {},
   "source": [
    "**Global config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a282ce0b-1aba-417c-bf9a-15cbe840c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_conf = {\n",
    "    \"embeddingmodel\": \"BAAI/bge-m3\",  # \"BAAI/bge-m3\" \"nomic-ai/nomic-embed-text-v2-moe\" # \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "    \"chunk_size\": 1536,\n",
    "    \"chunk_overlap\": 0,\n",
    "    \"top_similar\": 5,\n",
    "    \"n4rerank\": 0, #How many nodes to retrieve for reranking. If 0, reranker is not used\n",
    "    \"use_similar_chunks\": True, #To use similar chunks or the whole document as the context\n",
    "    \"prevnext\": True #to include in the context also the previouse and the next chunk of the current similar chunk\n",
    "}\n",
    "embedding=HuggingFaceEmbedding(model_name=embedding_conf[\"embeddingmodel\"],trust_remote_code=True)\n",
    "\n",
    "#For nomic-embed-text-v2-moe\n",
    "#embedding=HuggingFaceEmbedding(model_name=embedding_conf[\"embeddingmodel\"],trust_remote_code=True,query_instruction=\"search_query: \",text_instruction=\"search_document: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e114f1-c05e-48b1-b56a-051ece6102eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Setup \n",
    "llmmodelAzure = { \"model\": \"gpt-4o\",\n",
    "                \"version\":os.environ.get('AZURE_OPENAI_VERSION',''),\n",
    "                \"azure_deployment\":\"gpt-4o\",\n",
    "                \"azure_endpoint\":os.environ.get('AZURE_ENDPOINT',''),\n",
    "                \"api_key\":os.environ.get('AZURE_OPENAI_KEY','')}\n",
    "\n",
    "llm=AzureOpenAI(azure_deployment=llmmodelAzure[\"azure_deployment\"],\n",
    "                azure_endpoint=llmmodelAzure[\"azure_endpoint\"],temperature=0.0,\n",
    "                api_version=llmmodelAzure[\"version\"], api_key=llmmodelAzure[\"api_key\"],\n",
    "                timeout=120,max_retries=3,top_p=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f7c744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = Extractor() # Markdown doc extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e67876-c437-41ad-992b-4ce540022087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ollama model\n",
    "# llmmodelOllama = { \"model\": \"gemma3:27b\",\n",
    "#                 \"url\":os.environ.get('OLLAMA_ENDPOINT',''),\n",
    "#                 \"context_window\":\"128000\"}\n",
    "\n",
    "#from llama_index.llms.ollama import Ollama\n",
    "#llm = Ollama(base_url=llmmodelOllama[\"url\"],\n",
    "#             model=llmmodelOllama[\"model\"], \n",
    "#             context_window=int(llmmodelOllama[\"context_window\"]),\n",
    "#            request_timeout=300.0,\n",
    "#            temperature=0.0,\n",
    "#            additional_kwargs={\"seed\":1337})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0daa1-7706-4651-b713-78ac66c5533a",
   "metadata": {},
   "source": [
    "**PROCUREMENT FILE SETTINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88446367-533a-4e00-b3ff-239802a86698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions loaded\n"
     ]
    }
   ],
   "source": [
    "# Script dir for getting relative paths for notebook file\n",
    "script_dir = globals()['_dh'][0] \n",
    "\n",
    "# Document paths\n",
    "question_file_path = script_dir / \"questions\" / \"questions.yaml\"\n",
    "prompt_file = script_dir / \"questions\" / \"prompts.tsv\"\n",
    "report_dir = script_dir / \"reports\"\n",
    "config_dir = script_dir / \"demo_config\" # \"dev_config\" # \"config\"\n",
    "procurement_file_dir = script_dir / \"demo\" # \"cfla_files\"\n",
    "answer_file_dir = script_dir / \"answers\"\n",
    "\n",
    "# Loading static information TODO\n",
    "question_dictionary = get_questions(question_file_path)\n",
    "promptdict = get_prompt_dict(prompt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78357252",
   "metadata": {},
   "source": [
    "**MAIN Q/A GENERATION SCRIPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f5149a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 config files in C:\\Repos\\vpp-cfla\\demo_config\n",
      "Processing config file: C:\\Repos\\vpp-cfla\\demo_config\\GNP202565.ini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████████████████████████████████████████ [ time left: 00:00 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 segments created and vectorized.\n",
      "Index is ready.\n",
      "2 4 6 7 9 10 15 16 17 18 19 22 23 24 26 27 28 29 31 32 33 34 35 36 37 37.1 37.2 37.3 37.4 37.5 37.6 37.7 37.8 37.9 37.10 37.11 37.12 37.13 38 38.1 38.2 38.3 38.4 38.5 38.6 38.7 38.8 38.9 39 39.1 39.2 39.3 39.4 39.5 39.6 39.7 39.8 39.9 39.10 39.11 39.12 39.13 39.14 39.15 39.16 39.17 39.18 39.19 39.20 39.21 39.22 39.23 39.24 39.25 39.26 39.27 39.28 39.29 39.30 39.31 39.32 39.33 39.34 39.35 39.36 39.37 39.38 39.39 39.40 40 40.1 40.2 40.3 40.4 40.5 40.6 40.7 40.8 40.9 40.10 40.11 40.12 41 43 44 46 47 48 50 51 51.1 51.2 51.3 51.4 52 53 54 56 57 58 60 61 62 63 65 PRECIZITĀTE: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# TODO config file loop; For each config in config_folder_path; # TODO add parallel prompting\n",
    "# Create an array of ini files and then call them in a loop; This will give us more control over which files to call/exclude\n",
    "\n",
    "ini_files = [f for f in os.listdir(config_dir) if f.endswith('.ini')]\n",
    "print(f\"Found {len(ini_files)} config files in {config_dir}\")\n",
    "\n",
    "# TODO add a little indicator how many files are to be processes; + aproximate time to finish each file\n",
    "for file in ini_files:\n",
    "        configfile = config_dir / file\n",
    "        print(f\"Processing config file: {configfile}\")\n",
    "        procurement_id, procurement_file, agreement_file, answer_file = get_config_data(configfile, procurement_file_dir, answer_file_dir)\n",
    "        answer_dictionary = get_answers(answer_file)\n",
    "\n",
    "        # Getting markdown text from procurement doc\n",
    "        procurement_content = get_procurement_content(extractor, procurement_file, agreement_file)\n",
    "    \n",
    "        # Creating FAISS vector index for the procurement document\n",
    "        qnaengine = QnAEngine(embedding,llm)\n",
    "        await qnaengine.createIndex(\n",
    "                procurement_content,\n",
    "                \"Procurement\",\n",
    "                chunk_size=embedding_conf[\"chunk_size\"],\n",
    "                chunk_overlap=embedding_conf[\"chunk_overlap\"]\n",
    "                )\n",
    "\n",
    "        # Generating results\n",
    "        results_table = gen_results(qnaengine, configfile, embedding_conf, question_dictionary, answer_dictionary, promptdict)\n",
    "\n",
    "        # Save output\n",
    "\n",
    "        # TODO probably should merge the output file together; One output file for the entire run, that contains each specific procurement results\n",
    "        # TODO maybe implement some crash prevention strategy, so if some file breaks it doesn't break the entire run\n",
    "        data = pd.DataFrame(results_table, columns=[\"Nr\", \"Atbilde\", \"Sagaidāmā atbilde\", \"Pamatojums\"])\n",
    "        precision = (data['Atbilde'] == data['Sagaidāmā atbilde']).sum()/len(data)\n",
    "        print(f\"PRECIZITĀTE: {precision*100}%\")\n",
    "\n",
    "        # Save final output file\n",
    "        with open(f\"{report_dir}\\{date.today():%d.%m}_{procurement_id.replace('/','_')}.htm\", 'w', encoding='utf-8') as ofile:\n",
    "                print(data.to_html(index=False).replace('\\\\n','<br>'),file=ofile)\n",
    "                print(f\"PRECIZITĀTE: {precision*100}%\",file=ofile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
